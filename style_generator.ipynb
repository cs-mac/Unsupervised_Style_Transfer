{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"style_generator.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"51b8bd3365494278ab428a39c675cbc2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_07a1605096374c99ab9e27d6e07b2cbf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a31f434fbb2c4149bb16ceb83503eec1","IPY_MODEL_c23082ac0eb345998ab30f0938105bfb"]}},"07a1605096374c99ab9e27d6e07b2cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a31f434fbb2c4149bb16ceb83503eec1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dcc8ebc0ba5e480ba80604954a958edb","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fbc59e6bf71544f8946969c715c2f5eb"}},"c23082ac0eb345998ab30f0938105bfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c713ea98b51142fb8d54a3b59c647fbf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/5 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60cefda8a6694442823c29985ac7572c"}},"dcc8ebc0ba5e480ba80604954a958edb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fbc59e6bf71544f8946969c715c2f5eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c713ea98b51142fb8d54a3b59c647fbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"60cefda8a6694442823c29985ac7572c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"xbTEMpB2xmXb","colab_type":"text"},"source":["# Style-aware generation of text"]},{"cell_type":"code","metadata":{"id":"VakVk4B3PU68","colab_type":"code","colab":{}},"source":["# function ClickConnect(){\n","#     console.log(\"Working\");\n","#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n","# }\n","# setInterval(ClickConnect, 30000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLYilHX_lM1h","colab_type":"code","outputId":"2140a113-ea6f-4344-8437-16c9c91b9a04","executionInfo":{"status":"ok","timestamp":1592229080254,"user_tz":-120,"elapsed":20750,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# Mount Google Drive and go to correct directory\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8W2JcUmLlYk5","colab_type":"code","outputId":"b651286e-0916-4557-bad4-ba6df21bc53e","executionInfo":{"status":"ok","timestamp":1592229423471,"user_tz":-120,"elapsed":1115,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Navigate to the correct directory\n","%cd drive/\"My Drive\"/\"Colab Notebooks\"/master_project"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/master_project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0TyreU58nqbb","colab_type":"code","outputId":"3e53aeb5-3ab1-4c97-b95d-671e7a42353d","executionInfo":{"status":"ok","timestamp":1592229429300,"user_tz":-120,"elapsed":3374,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["import pickle\n","import tqdm\n","import time\n","import itertools\n","import math\n","import numpy as np\n","import keras\n","from keras.layers import Layer, Embedding, Input, Concatenate, Bidirectional, LSTM, TimeDistributed, Dense, Dropout, Lambda\n","from keras.models import Model\n","from keras import initializers, constraints, regularizers\n","from keras import backend as K\n","from keras.utils.np_utils import to_categorical\n","from keras.optimizers import Adam\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from operator import itemgetter\n","from IPython.display import HTML, display\n","from timeit import default_timer as timer\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.feature_extraction import stop_words\n","\n","from utilities import *\n","from get_antonyms import *"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NNlCVzyyQ0A4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2781b4c2-be60-4fc1-88d7-7605f086b1c0","executionInfo":{"status":"ok","timestamp":1592229349956,"user_tz":-120,"elapsed":3182,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}}},"source":["!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EA0X45VVxeHz","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"9AfKVu9SnFfM","colab_type":"code","colab":{}},"source":["# Load fully pre-preprocessed test data\n","with open(\"HAN/fully_processed.pickle\", \"rb\") as handle:\n","    *_, x_test, y_test, pos_test, is_set, word_index, pos_index= pickle.load(handle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATnBbz7wxidK","colab_type":"text"},"source":["## Model\n"]},{"cell_type":"code","metadata":{"id":"SSzo-aqun44r","colab_type":"code","colab":{}},"source":["# Attention Layer\n","# Adapted from https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n","def dot_product(x, kernel):\n","    \"\"\"\n","    Wrapper for dot product operation, in order to be compatibl|e with both\n","    Theano and Tensorflow\n","    Args:\n","        x (): input\n","        kernel (): weights\n","    Returns:\n","    \"\"\"\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","\n","class AttentionWithContext(Layer):\n","    \"\"\"\n","    Attention operation, with a context/query vector, for temporal data.\n","    Supports Masking.\n","    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n","    \"Hierarchical Attention Networks for Document Classification\"\n","    by using a context vector to assist the attention\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    How to use:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Note: The layer has been tested with Keras 2.0.6\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(AttentionWithContext())\n","        # next add a Dense layer (for classification/regression) or whatever...\n","    \"\"\"\n","\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight(shape=(input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight(shape=(input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        # apply mask after the exp. will be re-normalized next\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        # in some cases especially in the early stages of training the sum may be almost zero\n","        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n","        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO5fXe5x0B0A","colab_type":"code","colab":{}},"source":["max_senten_len = 100\n","embed_size = 300\n","max_senten_num = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3s8nCi1f011q","colab_type":"code","colab":{}},"source":["embedding_name = \"SKIP_negative10_EN_Change_inputSentiment_weightsum.p\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sD0X9Acz0za2","colab_type":"code","colab":{}},"source":["# Load word embedding matrix\n","embedding_name = embedding_name.split(\".\")[0]\n","with open(\"HAN/\"+embedding_name+\"_embedding_matrix.pickle\", \"rb\") as handle:\n","    embedding_matrix = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6G7MnDe0yN6","colab_type":"code","colab":{}},"source":["# Load POS-tag embedding matrix\n","pos_set = {'X', 'INTJ', 'PUNCT', 'PART', 'NUM', 'ADP', 'PROPN', 'ADV', 'SYM', 'ADJ', 'VERB', 'DET', 'PRON', 'AUX', 'CCONJ', 'NOUN'}\n","with open(\"HAN/pos_embedding_matrix.pickle\", \"rb\") as handle:\n","    pos_embedding_matrix = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82UZBWHI0lWQ","colab_type":"code","colab":{}},"source":["embedding_layer = Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], input_length=max_senten_len, trainable=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wB2ySNA80juT","colab_type":"code","colab":{}},"source":["embedding_pos = Embedding(len(word_index) + 1, len(pos_set), input_length=max_senten_len, trainable=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7-iQvLM2eQQ","colab_type":"code","colab":{}},"source":["# Regularization to reduce overfitting\n","REG_PARAM = 1e-13\n","l2_reg = regularizers.l2(REG_PARAM)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"giMy3uu-0iEm","colab_type":"code","colab":{}},"source":["# Individual parts of the model\n","\n","# Combined input has the shape => number of inputs * MAX_SENT_LENGTH\n","# NOTE: The seperation of individual inputs only works when num_inputs = 2, use different seperation method if more inputs are used\n","num_inputs = 2 \n","combined_input = Input(shape=(num_inputs*max_senten_len,), dtype='float32')\n","# Seperate word input\n","word_input = Lambda(lambda x: x[:, 0:max_senten_len])(combined_input)\n","# Get the word-embeddings\n","word_sequences = embedding_layer(word_input)\n","# Seperate the POS input\n","pos_input = Lambda(lambda x: x[:, max_senten_len: ])(combined_input)\n","pos_shaped = embedding_pos(pos_input)\n","# Concatenate the word embeddings with our POS-vectors\n","merged = Concatenate()([word_sequences, pos_shaped])\n","\n","word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(merged)\n","word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n","word_att = AttentionWithContext()(word_dense)\n","wordEncoder = Model(inputs=combined_input, outputs=word_att)\n","\n","sent_input = Input(shape=(max_senten_num, num_inputs*max_senten_len), dtype='float32')\n","sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n","sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n","sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n","sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n","preds = Dense(len(set(y_test)), activation='softmax')(sent_att)\n","model = Model(sent_input, preds)\n","\n","model.compile(loss='categorical_crossentropy',optimizer=Adam(0.1, amsgrad=True),metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLbHA3mVx1BL","colab_type":"text"},"source":["## Locate sentiment words"]},{"cell_type":"code","metadata":{"id":"6sRMgq2lpaIE","colab_type":"code","colab":{}},"source":["# Combine word and POS input\n","test_input = np.c_[x_test, pos_test]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43qGXI6dxcgL","colab_type":"code","outputId":"71eea605-bd1f-4c2e-f29d-79f92df7e0e7","executionInfo":{"status":"ok","timestamp":1592229458545,"user_tz":-120,"elapsed":1084,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":119,"referenced_widgets":["51b8bd3365494278ab428a39c675cbc2","07a1605096374c99ab9e27d6e07b2cbf","a31f434fbb2c4149bb16ceb83503eec1","c23082ac0eb345998ab30f0938105bfb","dcc8ebc0ba5e480ba80604954a958edb","fbc59e6bf71544f8946969c715c2f5eb","c713ea98b51142fb8d54a3b59c647fbf","60cefda8a6694442823c29985ac7572c"]}},"source":["tqdm.tnrange(5) # tqdm.notebook gives error without first trying tqdm normally"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51b8bd3365494278ab428a39c675cbc2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["  0%|<bar/>| 0/5 [00:00<?, ?it/s]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"e-M_15ujYLvX","colab_type":"code","colab":{}},"source":["model.load_weights(\"HAN/models/model1/han_trained_weights\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tx-xarzO4M21","colab_type":"code","colab":{}},"source":["def get_attention_weights(data_input, data_sentence, threshold=1): # len(test_input)\n","    '''\n","    Get the attention_weights for the words in one or more sentences\n","    '''\n","    for i, q in zip(tqdm.notebook.tnrange(threshold), range(threshold)):  \n","        get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output])\n","        out = get_layer_output([data_input[q],0])  \n","        eij = np.tanh(np.dot(out[0],model.layers[4].get_weights()[0]))\n","        t = np.dot(eij,model.layers[4].get_weights()[2])\n","        ai = np.exp(t)\n","        weights = ai/np.sum(ai)\n","        get_layer_op_words = K.function([wordEncoder.layers[0].input, K.learning_phase()], [wordEncoder.layers[7].output])\n","        op_words = get_layer_op_words([data_input[q],0])\n","\n","        weight_all_words = []\n","        for i in range(max_senten_num):\n","            eij_words = np.tanh(np.dot(op_words[0][i], wordEncoder.layers[8].get_weights()[0]))\n","            t_words = np.dot(eij_words,wordEncoder.layers[8].get_weights()[2])\n","            ai_words = np.exp(t_words)\n","            weights_words = ai_words/np.sum(ai_words)\n","            weight_all_words.append(weights_words)\n","\n","        id2word = {v: k for k, v in word_index.items()}    \n","        sent_number = 0\n","        sentence = []\n","        sentence_at = np.zeros(np.count_nonzero(data_sentence[q][0])) # since there is only one sentence we can extract its length\n","        for i in data_sentence[q]:\n","            weights_words = weight_all_words[sent_number]\n","            word_no = 0\n","            for j in i:\n","                if j!=0:\n","                    word = id2word[j]  \n","                    sentence.append(word)\n","                    sentence_at[word_no] = weights_words[word_no] # attention weight of word\n","                    # print(word, weights_words[word_no], sep=\"\\t\\t\")            \n","                word_no+=1\n","            sent_number+=1\n","        assert len(sentence) == len(sentence_at)\n","        yield sentence, sentence_at\n","        #print(*zip(sentence, softmax(sentence_at)), sep=\"\\n\", end=\"\\n\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cm0JXI4Xx9vF","colab_type":"code","colab":{}},"source":["# # Check if always same attention weights (if so model weights loaded succesfully)\n","# for i, j in get_attention_weights(test_input, x_test, threshold=2):\n","#     print(softmax(j))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wV_Rpk_Kx9oV","colab_type":"code","colab":{}},"source":["try:\n","    with open(\"sentence_attention_dict.pickle\", \"rb\") as handle:\n","       sentence_attention_dict = pickle.load(handle)\n","except FileNotFoundError as e:\n","    sentence_attention_dict = {}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5fzbLpPKe_t","colab_type":"code","colab":{}},"source":["with open(\"HAN/df_all.pkl\", \"rb\") as handle:\n","    df_all = pickle.load(handle)\n","test_sentences = df_all.words[df_all.is_which_set == \"test\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jetzPLYjP_zV","colab_type":"code","colab":{}},"source":["# Get the attention weights for all words of the test set\n","first = True\n","while len(sentence_attention_dict) < len(set(test_sentences)):\n","    start = timer()\n","    if not first:\n","        print(\"\\nOpening File...\")\n","        with open(\"sentence_attention_dict.pickle\", \"rb\") as handle:\n","            sentence_attention_dict = pickle.load(handle)\n","    print(f\"Start attention search... {len(sentence_attention_dict)}/{len(test_input)}\")\n","    remaining = len(sentence_attention_dict)  \n","    sent_att_gen = get_attention_weights(data_input=test_input[remaining:], data_sentence=x_test[remaining:], threshold=len(x_test[remaining:]))\n","    for idx, (sentence, attention) in enumerate(sent_att_gen):\n","        try:\n","            sentence_attention_dict[\" \".join(sentence)]\n","            print(\"Already in dict\")\n","        except KeyError as e:\n","            sentence_attention_dict[\" \".join(sentence)] = attention\n","        if idx == 2500:\n","            first = False\n","            break\n","    print(\"Saving File...\")\n","    with open(\"sentence_attention_dict.pickle\", \"wb\") as handle:\n","        pickle.dump(sentence_attention_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    time.sleep(5)\n","    print(f\"Minutes: {round((timer() - start)/60, 2)}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3uRGgHdu6EZ","colab_type":"code","colab":{}},"source":["# see directory pickles-5-5-2020\n","# def decide_subtitutes(sentence, attention, normalization=\"softmax\", threshold=0.85):\n","#     '''\n","#     Decide which words need to be substituted\n","#     '''\n","#     if normalization == \"softmax\":\n","#         attention = softmax(attention)\n","\n","#     try:\n","#         pos_tags = df_all.pos[df_all.words == sentence].item().split()\n","#     except ValueError as e:\n","#         pos_tags = df_all.pos[df_all.words == sentence].tolist()[0].split()\n","\n","#     sentence = sentence.split()\n","#     substitute_ids = []\n","#     for idx, (word, tag, score) in enumerate(zip(sentence, pos_tags, attention)):\n","#         # {'X', 'INTJ', 'PUNCT', 'PART', 'NUM', 'ADP', 'PROPN', 'ADV', 'SYM', 'ADJ', 'VERB', 'DET', 'PRON', 'AUX', 'CCONJ', 'NOUN'}\n","#         if len(word) > 2 and score >= threshold and tag not in [\"NOUN\", \"DET\", \"PUNCT\", \"NUM\", \"PROPN\", \"CCONJ\"]:\n","#             substitute_ids.append((idx, score))\n","\n","#     if substitute_ids == []:\n","#         for idx, (word, tag, score) in enumerate(zip(sentence, pos_tags, attention)):\n","#             if len(word) > 2 and tag == \"ADJ\" and score >= 0.35:\n","#                 substitute_ids.append((idx, score))\n","\n","#     if substitute_ids == []:\n","#         for idx, (word, tag, score) in enumerate(zip(sentence, pos_tags, attention)):\n","#             if len(word) > 2 and (tag == \"ADV\" or tag == \"VERB\"):\n","#                 substitute_ids.append((idx, score))\n","#         if len(substitute_ids) > 2:\n","#             substitute_ids = sorted(substitute_ids, key=itemgetter(1), reverse=True)\n","#             if substitute_ids[1][0] >= 0.7:\n","#                 substitute_ids = substitute_ids[:2]\n","#             else:\n","#                 substitute_ids = substitute_ids[0]\n","\n","#     if substitute_ids == []:\n","#         for idx, word in enumerate(sentence):\n","#             if word == \"not\":\n","#                 substitute_ids.append((idx, 0))\n","#             break\n","\n","#     return substitute_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8pitmghb7P0","colab_type":"code","colab":{}},"source":["def decide_subtitutes(sentence, attention, normalization=\"softmax\", threshold=0.7):\n","    '''\n","    Decide which words need to be substituted\n","    '''\n","    if normalization == \"softmax\":\n","        attention = softmax(attention)\n","\n","    try:\n","        pos_tags = df_all.pos[df_all.words == sentence].item().split()\n","    except ValueError as e:\n","        pos_tags = df_all.pos[df_all.words == sentence].tolist()[0].split()\n","\n","    sentence = sentence.split()\n","    substitute_ids = []\n","    for idx, (word, tag, score) in enumerate(zip(sentence, pos_tags, attention)):\n","        # {'X', 'INTJ', 'PUNCT', 'PART', 'NUM', 'ADP', 'PROPN', 'ADV', 'SYM', 'ADJ', 'VERB', 'DET', 'PRON', 'AUX', 'CCONJ', 'NOUN'}\n","        # see directory pickles-5-5-2020\n","        # if len(word) > 2 and tag not in [\"NOUN\", \"DET\", \"PUNCT\", \"NUM\", \"PROPN\"]:\n","        #     substitute_ids.append((idx, score))\n","\n","        if len(word) > 2 and tag not in [\"NOUN\", \"DET\", \"PUNCT\", \"NUM\", \"PROPN\", \"CCONJ\", \"X\"]:\n","            substitute_ids.append((idx, score))\n","\n","    if len(substitute_ids) > 1:\n","        substitute_ids = sorted(substitute_ids,key=itemgetter(1), reverse=True)\n","        substitute_ids = [substitute_ids[0]] + [(id, score) for id, score in substitute_ids[1:] if score >= threshold]\n","\n","    return substitute_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkaUrwg7a2nZ","colab_type":"code","colab":{}},"source":["with open(\"sentence_attention_dict.pickle\", \"rb\") as handle:\n","    sentence_attention_dict = pickle.load(handle)\n","\n","try:\n","    with open(\"to_substitute_dict.pickle\", \"rb\") as handle:\n","        to_substitute_dict = pickle.load(handle)\n","except FileNotFoundError as e:\n","    to_substitute_dict = {}\n","    with open(\"to_substitute_dict.pickle\", \"wb\") as handle:\n","        pickle.dump(to_substitute_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","if len(sentence_attention_dict) != len(to_substitute_dict):\n","    for idx, (k, v) in enumerate(tqdm.tqdm(sentence_attention_dict.items())):\n","        try:\n","            to_substitute_dict[k]\n","        except KeyError as e:\n","            to_substitute_dict[k] = decide_subtitutes(k, v)\n","        if idx+1 in split_into_parts(len(sentence_attention_dict.items()), 4):\n","            with open(\"to_substitute_dict.pickle\", \"wb\") as handle:\n","                pickle.dump(to_substitute_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3sfY_Lcxs4-","colab_type":"text"},"source":["## Word Substitution\n"]},{"cell_type":"code","metadata":{"id":"ZbLYaS3jxmX1","colab_type":"code","outputId":"f0e56c63-d796-48c9-d200-7f720f93bd37","executionInfo":{"status":"ok","timestamp":1589222783920,"user_tz":-120,"elapsed":63907,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["test_sentence1 = \"it is very dense solid tastes like dark chocolate but its heaviness does not leave a good taste in your mouth\"\n","print(softmax(sentence_attention_dict[test_sentence1]))\n","print(\"\\n\",to_substitute_dict[test_sentence1])\n","for idx, att in to_substitute_dict[test_sentence1]:\n","    print(test_sentence1.split()[idx])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.664 0.047 0.2   0.987 0.883 0.686 0.164 0.169 0.652 0.484 0.521 0.433\n"," 0.164 0.059 0.21  0.884 0.738 0.629 0.794 0.626 0.413]\n","\n"," [(3, 0.987), (4, 0.883), (16, 0.738)]\n","dense\n","solid\n","good\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-fFMK_D6T3JJ","colab_type":"code","outputId":"374d85bb-36c6-45f7-8e16-ec929c4f458d","executionInfo":{"status":"ok","timestamp":1589222784576,"user_tz":-120,"elapsed":64502,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["with open(\"antonym_dict.pickle\", \"rb\") as handle:\n","    antonym_dict = pickle.load(handle)\n","print(f\"Currently {len(antonym_dict)} antonyms stored\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Currently 8854 antonyms stored\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aIlvyLZgXNVr","colab_type":"code","colab":{}},"source":["try:\n","    with open(\"sentence_generatedsentence_dict.pickle\", \"rb\") as handle:\n","        sentence_generatedsentence_dict = pickle.load(handle)\n","except FileNotFoundError as e:\n","    sentence_generatedsentence_dict = {}\n","    with open(\"sentence_generatedsentence_dict.pickle\", \"wb\") as handle:\n","        pickle.dump(sentence_generatedsentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLMqpMpbQvP7","colab_type":"code","outputId":"4a5ab9eb-5cd4-4598-d0c5-bac645d33011","executionInfo":{"status":"ok","timestamp":1589222785591,"user_tz":-120,"elapsed":65439,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Test if scraping TheSauri works fine\n","assert power_thesaurus(\"warm\", inspect=True)  == ['cool', 'chill', 'unfriendly', 'cold', 'frigid', 'unenthusiastic', 'harsh', 'atrocious', 'hostile', 'dull']\n","assert thesaurus(\"warm\", inspect=True) == ['calm', 'cold', 'cool', 'freezing', 'stormy', 'unhappy', 'violent']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['cool', 'chill', 'unfriendly', 'cold', 'frigid', 'unenthusiastic', 'harsh', 'atrocious', 'hostile', 'dull']\n","['calm', 'cold', 'cool', 'freezing', 'stormy', 'unhappy', 'violent']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rK_3VnhBV213","colab_type":"code","outputId":"04c2a5d2-020c-4715-b6ac-2f8c574d1426","executionInfo":{"status":"ok","timestamp":1589234847606,"user_tz":-120,"elapsed":5550377,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["while len(sentence_generatedsentence_dict) != len(to_substitute_dict):\n","    start = timer()\n","    print(f\"\\nStart generating sentences... {len(sentence_generatedsentence_dict)}/{len(to_substitute_dict)}\")\n","    # Ensure items that are already generated are skipped, since Python 3.6+ dicts are ordered according to insertion order\n","    for idx, (sentence, substitute_ids) in enumerate(itertools.islice(to_substitute_dict.items(), len(sentence_generatedsentence_dict), None)):\n","        generated_sentence = sentence.split()\n","        if type(substitute_ids) == tuple:\n","            substitute_ids = [substitute_ids]\n","        if len(substitute_ids) > 1:\n","            for id, score in substitute_ids:\n","                to_replace = generated_sentence[id]\n","                if to_replace == \"not\":\n","                    generated_sentence[id] = \"\"\n","                else:\n","                    antonym = get_antonym(to_replace)\n","                    generated_sentence[id] = antonym\n","        elif len(substitute_ids) == 1:\n","            to_replace = generated_sentence[substitute_ids[0][0]]\n","            if to_replace == \"not\":\n","                generated_sentence[substitute_ids[0][0]] = \"\"\n","            else:\n","                antonym = get_antonym(to_replace)\n","                generated_sentence[substitute_ids[0][0]] = antonym\n","        else:\n","            generated_sentence = sentence.split()\n","        sentence_generatedsentence_dict[sentence] = \" \".join(generated_sentence)\n","        if idx == 2500:\n","            break\n","    print(\"Saving File...\")\n","    with open(\"sentence_generatedsentence_dict.pickle\", \"wb\") as handle:\n","        pickle.dump(sentence_generatedsentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    time.sleep(5)\n","    print(f\"Minutes: {round((timer() - start)/60, 2)}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Start generating sentences... 65026/99839\n","Saving File...\n","Minutes: 15.61\n","\n","Start generating sentences... 67527/99839\n","Saving File...\n","Minutes: 14.94\n","\n","Start generating sentences... 70028/99839\n","Saving File...\n","Minutes: 14.51\n","\n","Start generating sentences... 72529/99839\n","Saving File...\n","Minutes: 14.27\n","\n","Start generating sentences... 75030/99839\n","Saving File...\n","Minutes: 14.38\n","\n","Start generating sentences... 77531/99839\n","Saving File...\n","Minutes: 14.23\n","\n","Start generating sentences... 80032/99839\n","Saving File...\n","Minutes: 14.36\n","\n","Start generating sentences... 82533/99839\n","Saving File...\n","Minutes: 14.23\n","\n","Start generating sentences... 85034/99839\n","Saving File...\n","Minutes: 14.54\n","\n","Start generating sentences... 87535/99839\n","Saving File...\n","Minutes: 14.63\n","\n","Start generating sentences... 90036/99839\n","Saving File...\n","Minutes: 14.4\n","\n","Start generating sentences... 92537/99839\n","Saving File...\n","Minutes: 13.69\n","\n","Start generating sentences... 95038/99839\n","Saving File...\n","Minutes: 14.31\n","\n","Start generating sentences... 97539/99839\n","Saving File...\n","Minutes: 12.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sc_-EVp86QjM","colab_type":"code","outputId":"20ee0ceb-58a5-4e66-8f8a-c36aa3c92f39","executionInfo":{"status":"ok","timestamp":1589240587803,"user_tz":-120,"elapsed":816,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(len(sentence_generatedsentence_dict), len(to_substitute_dict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99839 99839\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMw8ZzuvG836","colab_type":"code","colab":{}},"source":["# Evaluation DataFrame\n","# eval_df =\n","# OG_sentence | OG_sentiment | GENERATED_sentence | GENERATED_sentiment"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnRPCtfe4Nzu","colab_type":"text"},"source":["## Visualization of attention weights\n"]},{"cell_type":"code","metadata":{"id":"APOt9nwYy8Fw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1724584e-93f2-4df7-e17f-2d2b76fa160c","executionInfo":{"status":"ok","timestamp":1592231238141,"user_tz":-120,"elapsed":62626,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}}},"source":["# Visualization of word attention\n","# model = loaded_model\n","cmap_sent = plt.cm.Oranges\n","cmap = plt.cm.Blues\n","\n","def progress(value, max=100):\n","    return HTML(\"\"\"\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(value=value, max=max))\n","\n","sentence_language = 0\n","sentence_pos = 0\n","\n","threshold = 100\n","pbar = display(progress(0, threshold), display_id=True)\n","with open(\"visualization.html\", \"w\") as html_file:\n","    html_file.write('<!DOCTYPE html>\\n')\n","    html_file.write('<html>\\n')\n","    html_file.write('<body>\\n')\n","    for q in range(len(test_input)):\n","        if q >= threshold:\n","            break   \n","        get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output])\n","        out = get_layer_output([test_input[q],0])  \n","        eij = np.tanh(np.dot(out[0],model.layers[4].get_weights()[0]))\n","        t = np.dot(eij,model.layers[4].get_weights()[2])\n","        ai = np.exp(t)\n","        weights = ai/np.sum(ai)\n","        get_layer_op_words = K.function([wordEncoder.layers[0].input, K.learning_phase()], [wordEncoder.layers[7].output])\n","        op_words = get_layer_op_words([test_input[q],0])\n","\n","        weight_all_words = []\n","        for i in range(max_senten_num):\n","            eij_words = np.tanh(np.dot(op_words[0][i], wordEncoder.layers[8].get_weights()[0]))\n","            t_words = np.dot(eij_words,wordEncoder.layers[8].get_weights()[2])\n","            ai_words = np.exp(t_words)\n","            weights_words = ai_words/np.sum(ai_words)\n","            weight_all_words.append(weights_words)\n","\n","        id2word = {v: k for k, v in word_index.items()}    \n","\n","        sent_no = 0\n","        languages = [\"Language\", \"Pos\"]\n","        html_file.write('<b>Review (%s)</b> <br>' % q)\n","        for i, lang in zip(x_test[q], languages):\n","            weights_words = weight_all_words[sent_no]\n","            template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n","            sent_template = '{} <span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n","            sent = ''\n","            word_no = 0\n","            if lang == \"Language\":\n","                sentence_language += float(weights[0][sent_no])\n","            elif lang == \"Pos\":\n","                sentence_pos += float(weights[0][sent_no])\n","            sent_color = matplotlib.colors.rgb2hex(cmap_sent(float(weights[0][sent_no])*0.75)[:3])\n","            html_file.write(sent_template.format(\"Sentence (0)\", sent_color, '&nbsp' + str(weights[0][sent_no]) + '&nbsp'))     \n","            summed_weights = 0\n","            np_color = []\n","            for j in i:\n","                if j!=0:\n","                    alpha = weights_words[word_no]\n","                    np_color.append(alpha)\n","                    summed_weights += alpha\n","                word_no+=1\n","            word_no = 0\n","            np_color = softmax(np.array(np_color)) # normalize between 0->1\n","            for j, color in zip(i, np_color):\n","                if j!= 0:\n","                    color = matplotlib.colors.rgb2hex(cmap(float(color)*0.9)[:3])\n","                    word = id2word[j]\n","                    html_file.write(template.format(color, '&nbsp' + word + '&nbsp'))\n","                    sent+=id2word[j]+' '+str(weights_words[word_no])+' ' \n","                word_no+=1\n","            html_file.write('<br>')\n","            sent_no+=1\n","        html_file.write('<br>')\n","        pbar.update(progress(q, threshold))\n","    html_file.write('</body>\\n')\n","    html_file.write('</html>\\n')     "],"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","        <progress\n","            value='99'\n","            max='100',\n","            style='width: 100%'\n","        >\n","            99\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"CMtzAlFSV-YO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"2f3b82b0-0ea7-4bce-8048-b7cafc85498f","executionInfo":{"status":"ok","timestamp":1592230930109,"user_tz":-120,"elapsed":1012,"user":{"displayName":"moonreaderx","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSaH853EhXiIWVIbhKdw0jYZM-DFsphxsUEahJ=s64","userId":"16509038249165111128"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","mat = np.array([[0,0.49508174, 0.08274638, 0.59135684, 0.90611644, 0.37030567, 0.04172206, \n","                0.66000069, 0.83250818, 0.57374382, 0.18245247, 1]])\n","plt.imshow(mat, origin=\"lower\", cmap='Blues', interpolation='nearest')\n","plt.colorbar()\n","plt.show()"],"execution_count":75,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}